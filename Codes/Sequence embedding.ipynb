{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a96b9b-280f-4a56-ac3c-0b7c52b45737",
   "metadata": {},
   "source": [
    "Extracting protein sequences' features using pretrained-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cfb61c-4f32-4c93-b7d0-20c889287672",
   "metadata": {},
   "source": [
    "1.Converting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c283eaa-9893-4157-b9c9-1ca3afbeb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "def fasta_to_excel(fasta_file, excel_file):\n",
    "\n",
    "    records = SeqIO.parse(fasta_file, \"fasta\")\n",
    "    \n",
    "    \n",
    "    data = {\"ID\": [], \"Sequence\": []}\n",
    "    for record in records:\n",
    "        protein_name = record.id\n",
    "        sequence = str(record.seq)\n",
    "        data[\"ID\"].append(protein_name)\n",
    "        data[\"Sequence\"].append(sequence)\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    \n",
    "    df.to_excel(excel_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf52fa0-cdf3-4b22-8dc0-1d42f489755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"  \"\n",
    "excel_file = \"  \"\n",
    "fasta_to_excel(fasta_file, excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27c707-6333-4f3c-a15c-4021474bd593",
   "metadata": {},
   "source": [
    "2.Creating or loading sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a4d08-6c2e-4e68-87d8-de860c814ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.read_excel('  ',na_filter = False) \n",
    "sequence_list = dataset['Sequence'].apply(lambda x: x[:1000] if len(x) > 1000 else x)\n",
    "sequence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9759c2f-7b22-4566-bd8a-35a612735b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_sequence_list = []\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] \n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list.append(tuple_sequence) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323e7d1-fcfd-4c8d-9465-f6d7381f9a7c",
   "metadata": {},
   "source": [
    "3.Tokenize, encoding sequences and loading the ESM pretrained-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32030a63-b8d5-45e0-9a55-f102d81aa77f",
   "metadata": {},
   "source": [
    "Loading the esm1_t6_43M_UR50S model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a27f5-15d9-43de-a876-0390ba243c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1_t6_43M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][6]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135318f4-6f53-4d3f-9190-cbc0e9c24d1a",
   "metadata": {},
   "source": [
    "Loading the esm1_t34_670M_UR50S model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be264e41-9258-4176-9b04-a5f7ad5ea28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][6]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ac0d5-c2df-4ba6-8d3e-441a07ded967",
   "metadata": {},
   "source": [
    "Loading the esm1_t12_85M_UR50S model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e865e-d5a8-425f-ac70-bac3d9f86342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1_t12_85M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[12], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][12]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615be109-66a9-41cf-a706-e73ad822b96e",
   "metadata": {},
   "source": [
    "Loading the esm1_t34_670M_UR50D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882761c-5a24-4276-b046-d28e807578b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1_t34_670M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[34], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][34]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7cbc9-4814-4435-a992-6b2edf20e61c",
   "metadata": {},
   "source": [
    "Loading the esm1_t34_670M_UR50D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cfe14-f46b-42a5-bb00-836da816c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[34], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][34]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e3e12-520d-4ccd-a859-08d5951eb094",
   "metadata": {},
   "source": [
    "Loading the esm1b_t33_650M_UR50S model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e9920-6e3e-4bf6-9aeb-7baaef050662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[33], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][33]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10999f6-987d-4340-bc95-ad21a967926f",
   "metadata": {},
   "source": [
    "Loading the esm2_t6_8M_UR50D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e2ea6-eca9-4cfd-8376-53e37593748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][6]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993b5b6-3ebf-4958-9c64-04df816c4a21",
   "metadata": {},
   "source": [
    "Loading the esm2_t12_35M_UR50D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b1df4-3831-4a68-b8b5-67fb33e170dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[12], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][12]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ee2b4-9f00-49df-80b8-37638be76358",
   "metadata": {},
   "source": [
    "Loading the esm2_t33_650M_UR50D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4822f9-adbb-48f6-9ef3-432aaaed5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  import torch\n",
    "  import esm\n",
    "  import pandas as pd\n",
    "  import collections\n",
    "  # 加载模型\n",
    "  model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  \n",
    "\n",
    "  embeddings_results = []\n",
    "  for peptide_sequence in peptide_sequence_list:\n",
    "      # 加载数据\n",
    "      batch_labels, batch_strs, batch_tokens = batch_converter([peptide_sequence])\n",
    "      batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "\n",
    "      \n",
    "      with torch.no_grad():\n",
    "          results = model(batch_tokens, repr_layers=[33], return_contacts=True)  \n",
    "      token_representations = results[\"representations\"][33]\n",
    "      sequence_representations = []\n",
    "      for i, tokens_len in enumerate(batch_lens):\n",
    "          sequence_representations.append(token_representations[i, 1:tokens_len-1].mean(0))\n",
    "      \n",
    "      sequence_embeddings = []\n",
    "      for i in range(len(sequence_representations)):\n",
    "          each_seq_rep = sequence_representations[i].tolist()\n",
    "          sequence_embeddings.extend(each_seq_rep)\n",
    "      \n",
    "      embeddings_results.append(sequence_embeddings)\n",
    "  \n",
    "  embeddings_results = pd.DataFrame(embeddings_results)\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e714e4c-e4c3-4bf8-96ff-361bc27b4886",
   "metadata": {},
   "source": [
    "4.Extracting sequences' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e706504-104e-4345-82c9-a980f68ceadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_results = esm_embeddings(peptide_sequence_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
